# DataLakeHouse - Olist E-commerce Analytics

## ðŸ“‹ Tá»•ng quan dá»± Ã¡n

Há»‡ thá»‘ng Data Lakehouse xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ Olist sá»­ dá»¥ng kiáº¿n trÃºc Medallion (Bronze â†’ Silver â†’ Gold) vá»›i Apache Spark, Delta Lake, vÃ  MinIO.

### ðŸŽ¯ Má»¥c tiÃªu
- XÃ¢y dá»±ng pipeline ETL tá»± Ä‘á»™ng hÃ³a vá»›i Change Data Capture (CDC)
- Ãp dá»¥ng kiáº¿n trÃºc Medallion Ä‘á»ƒ quáº£n lÃ½ cháº¥t lÆ°á»£ng dá»¯ liá»‡u
- PhÃ¢n tÃ­ch hÃ nh vi khÃ¡ch hÃ ng vÃ  hiá»‡u suáº¥t kinh doanh
- TÃ­ch há»£p cÃ¡c cÃ´ng cá»¥ BI/Analytics hiá»‡n Ä‘áº¡i

## ðŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL  â”‚â”€CDCâ”€>â”‚  Kafka   â”‚â”€â”€â”€â”€â”€>â”‚   MinIO   â”‚â”€â”€â”€â”€â”€>â”‚  Metabase  â”‚
â”‚  (Source)   â”‚      â”‚ Debezium â”‚      â”‚  S3 Lake  â”‚      â”‚    (BI)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                                         â”‚  Spark   â”‚
                                         â”‚Processingâ”‚
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                              â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚                        â”‚                        â”‚
                â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                â”‚  Bronze  â”‚          â”‚    Silver    â”‚         â”‚    Gold     â”‚
                â”‚  (Raw)   â”‚          â”‚  (Cleansed)  â”‚         â”‚(Aggregated) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ› ï¸ Stack cÃ´ng nghá»‡

| ThÃ nh pháº§n | CÃ´ng nghá»‡ | PhiÃªn báº£n | Port | Má»¥c Ä‘Ã­ch |
|------------|-----------|-----------|------|----------|
| **Source DB** | PostgreSQL | 15 | 5432 | Database gá»‘c |
| **Messaging** | Apache Kafka (KRaft) | 3.7.0 | 9092 | Streaming platform |
| **CDC** | Debezium Connect | 2.7.0 | 8083 | Change Data Capture |
| **Storage** | MinIO | latest | 9000/9001 | S3-compatible object storage |
| **Processing** | Apache Spark | 3.5.1 | 7077/8080 | Distributed processing |
| **Table Format** | Delta Lake | 3.2.0 | - | ACID transactions |
| **Orchestration** | Apache Airflow | 2.9.0 | 8081 | Workflow scheduling |
| **Query Engine** | Trino | latest | 8082 | Distributed SQL query |
| **Metastore** | Hive Metastore | 4.0.0 | 9083 | Metadata management |
| **BI** | Metabase | latest | 3000 | Business Intelligence |
| **Monitoring** | Kafka UI | latest | 8084 | Kafka monitoring |

## ðŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```
DataLakeHouse/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ bronze_to_silver_dag.py     
â”‚   â””â”€â”€ logs/                             
â”œâ”€â”€ connectors/
â”‚   â”œâ”€â”€ postgres-olist-initial.json      
â”‚   â””â”€â”€ register-connectors.sh            
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ *.csv                             
â”‚   â””â”€â”€ import_raw.sql                   
â”œâ”€â”€ minio_data/
â”‚   â”œâ”€â”€ bronze/                          
â”‚   â””â”€â”€ silver/                           
â”œâ”€â”€ postgres_data/                       
â”œâ”€â”€ spark/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ process_bronze_to_silver.py   
â”‚       â”œâ”€â”€ show_customers.py             
â”‚       â””â”€â”€ validate_silver_quality.py    
â”œâ”€â”€ Script/
â”‚   â”œâ”€â”€ create_tables.sql                
â”‚   â””â”€â”€ import_raw.sql                   
â”œâ”€â”€ trino/
â”‚   â”œâ”€â”€ catalog/                       
â”‚   â””â”€â”€ config.properties                
â”œâ”€â”€ hive/
â”‚   â””â”€â”€ hive-site.xml                     
â””â”€â”€ docker-compose.yml                    
```

## ðŸš€ HÆ°á»›ng dáº«n cÃ i Ä‘áº·t

### 1. Prerequisites
```bash
# YÃªu cáº§u há»‡ thá»‘ng
- Docker Desktop 20+
- Docker Compose 2+
- Git
- 16GB RAM (khuyáº¿n nghá»‹)
- 50GB disk space
```

### 2. Clone repository
```bash
git clone https://github.com/koi132/DataLakeHouse.git
cd DataLakeHouse
```

### 3. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng
```bash
# Start all services (build if needed)
docker-compose up --build -d

# Verify all services are running
docker ps
```

### 4. Táº¡o databases
```bash
# Create Airflow database
docker exec -it postgres psql -U postgres -c "CREATE DATABASE airflow;"

# Create Metabase database
docker exec -it postgres psql -U postgres -c "CREATE DATABASE metabase;"
```

### 5. Import dá»¯ liá»‡u Olist

**Step 1: Copy files vÃ o PostgreSQL container**
```bash
# Copy dataset CSV files
docker cp dataset/. postgres:/tmp/

# Copy SQL scripts
docker cp Script/. postgres:/tmp/
```

**Step 2: Táº¡o tables trong database orders**
```bash
# Access PostgreSQL container
docker exec -it postgres bash

# View create tables script
cat /tmp/create_tables.sql

# Execute DDL script
psql -U postgres -d orders -f /tmp/create_tables.sql

# Exit container
exit
```

**Step 3: Import dá»¯ liá»‡u vÃ o tables**
```bash
# Access PostgreSQL container
docker exec -it postgres bash

# View import script
cat /tmp/import_raw.sql

# Execute import script
psql -U postgres -d orders -f /tmp/import_raw.sql

# Exit container
exit
```

### 6. ÄÄƒng kÃ½ Debezium CDC Connector

**Register connector Ä‘á»ƒ capture changes**
```bash
# Navigate to connectors directory
cd connectors

# Register PostgreSQL source connector
bash register-connectors.sh
```

**Verify connector status**
```bash
# Check connector via API
curl http://localhost:8083/connectors/postgres-olist-source/status

# Or check via Kafka UI
# Open: http://localhost:8084
# Navigate to: Kafka Connect â†’ Connectors
```

### 7. Initial Bulk Load vÃ  CDC

**Kiá»ƒm tra Kafka topics**
```
Open Kafka UI: http://localhost:8084
Navigate to: Topics

Expected topics (9 tables):
- olist.public.olist_customers
- olist.public.olist_orders
- olist.public.olist_order_items
- olist.public.olist_order_payments
- olist.public.olist_order_reviews
- olist.public.olist_products
- olist.public.olist_sellers
- olist.public.olist_geolocation
- olist.public.product_category_translation
```

**Test CDC vá»›i insert máº«u**
```bash
# Connect to PostgreSQL
docker exec -it postgres psql -U postgres -d orders

# Insert test data
INSERT INTO olist_customers (customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state)
VALUES ('test123', 'unique123', '12345', 'Test City', 'SP');

# Verify in Kafka UI - check topic messages
```

### 8. Bronze Layer - Kafka to MinIO

**Cháº¡y streaming job Ä‘á»ƒ Ä‘Æ°a data tá»« Kafka sang Bronze layer**
```bash
# Access Spark master container
docker exec -it spark-master bash

# Submit Spark streaming job
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,\
org.apache.kafka:kafka-clients:3.5.1,\
org.apache.hadoop:hadoop-aws:3.3.2,\
com.amazonaws:aws-java-sdk-bundle:1.12.262,\
io.delta:delta-spark_2.12:3.2.0 \
  /opt/spark/app/stream_kafka_to_bronze.py
```

**Verify Bronze data in MinIO**
```
Open MinIO Console: http://localhost:9001
Credentials: admin / password123
Navigate to: Buckets â†’ bronze

Expected structure:
bronze/
â”œâ”€â”€ olist.public.olist_customers/
â”œâ”€â”€ olist.public.olist_orders/
â”œâ”€â”€ olist.public.olist_order_items/
â””â”€â”€ ... (9 tables total)
```

### 9. Silver Layer - Bronze to Silver ETL

**Option 1: Manual execution**
```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --master local[*] \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/process_bronze_to_silver.py
```

**Option 2: Via Airflow UI**
```
Open Airflow: http://localhost:8081
Credentials: hoang / 123456
DAG: bronze_to_silver_processing
Click: Trigger DAG
```

**Verify Silver data**
```bash
# Via MinIO Console
# Navigate to: silver/ bucket

# Via PySpark script
docker exec spark-master /opt/spark/bin/spark-submit \
  --master local[*] \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/show_customers.py
# Username: hoang / Password: 123456
# Trigger DAG: bronze_to_silver_processing
```

## ðŸ“Š Bronze Layer (Raw Data)

### MÃ´ táº£
- **Äá»‹nh dáº¡ng**: Parquet (khÃ´ng nÃ©n)
- **Schema**: Giá»¯ nguyÃªn tá»« CDC events
- **Äáº·c Ä‘iá»ƒm**: Immutable, append-only
- **Use case**: Data archival, replay capability

### Báº£ng dá»¯ liá»‡u
| Báº£ng | Records | MÃ´ táº£ |
|------|---------|-------|
| `olist.public.olist_customers` | ~99,441 | ThÃ´ng tin khÃ¡ch hÃ ng |
| `olist.public.olist_orders` | ~99,441 | ÄÆ¡n hÃ ng |
| `olist.public.olist_order_items` | ~112,650 | Chi tiáº¿t sáº£n pháº©m trong Ä‘Æ¡n |
| `olist.public.olist_order_payments` | ~103,886 | Thanh toÃ¡n |
| `olist.public.olist_order_reviews` | ~100,000 | ÄÃ¡nh giÃ¡ khÃ¡ch hÃ ng |
| `olist.public.olist_products` | ~32,951 | Sáº£n pháº©m |
| `olist.public.olist_sellers` | ~3,095 | NgÆ°á»i bÃ¡n |
| `olist.public.olist_geolocation` | ~1,000,516 | Tá»a Ä‘á»™ Ä‘á»‹a lÃ½ |
| `olist.public.product_category_translation` | 71 | Dá»‹ch danh má»¥c |

### Truy váº¥n Bronze data
```bash
# Via Trino
docker exec -it trino trino --catalog hive --schema default



## ðŸ¥ˆ Silver Layer (Cleansed Data)

### Äáº·c Ä‘iá»ƒm
- **Äá»‹nh dáº¡ng**: Delta Lake (ACID transactions)
- **Schema**: Standardized, validated, enriched
- **Cháº¥t lÆ°á»£ng**: Cleaned, deduplicated, business rules applied
- **Storage**: MinIO (s3a://silver/)
- **Compression**: Snappy
- **Total Size**: 109 MiB (131 objects)
- **Total Records**: ~570,000

### Quy trÃ¬nh xá»­ lÃ½ (Bronze â†’ Silver)

**Script**: `spark/app/process_bronze_to_silver.py`

**CÃ¡c bÆ°á»›c thá»±c hiá»‡n**:
1. **Deduplication**: Loáº¡i bá» duplicate records theo primary keys
2. **Data Cleansing**: Chuáº©n hÃ³a text (uppercase, trim whitespace)
3. **Type Casting**: Chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u phÃ¹ há»£p
4. **Business Logic**: TÃ­nh toÃ¡n metrics vÃ  derived columns
5. **Validation**: Kiá»ƒm tra business rules vÃ  constraints
6. **Enrichment**: ThÃªm metadata (processed_at timestamp)

### Transformations chi tiáº¿t

#### 1. **olist_customers** (99,441 records)
**Primary Key**: `customer_id`

**Transformations**:
- âœ… Deduplication báº±ng Window function + row_number() theo `customer_id`
- âœ… Uppercase: `customer_city`, `customer_state`
- âœ… Trim whitespace cho táº¥t cáº£ string columns
- âœ… Add `processed_at` timestamp

**Business Rules**:
```python
# Chá»‰ giá»¯ record má»›i nháº¥t cho má»—i customer_id
window = Window.partitionBy("customer_id").orderBy(desc("processed_at"))
df = df.withColumn("row_num", row_number().over(window))
df = df.filter(col("row_num") == 1).drop("row_num")
```

**Sample Output**:
```
customer_id | customer_unique_id | customer_zip_code_prefix | customer_city | customer_state
------------|-------------------|-------------------------|---------------|---------------
abc123...   | xyz789...         | 01310                   | SÃƒO PAULO     | SP
```

#### 2. **olist_orders** (99,441 records)
**Primary Key**: `order_id`

**New Columns**:
- `order_year`: Year tá»« `order_purchase_timestamp`
- `order_month`: Month (1-12)
- `order_day`: Day of month (1-31)
- `order_hour`: Hour of day (0-23)
- `approval_delay_days`: Thá»i gian tá»« mua â†’ duyá»‡t (purchase â†’ approved)
- `actual_delivery_days`: Thá»i gian giao thá»±c táº¿ (purchase â†’ delivered)
- `estimated_delivery_days`: Thá»i gian giao Æ°á»›c tÃ­nh (purchase â†’ estimated)
- `delivery_delay_days`: Sá»‘ ngÃ y giao muá»™n (actual - estimated), min = 0
- `is_delivered_late`: Boolean (TRUE náº¿u giao muá»™n)
- `is_delivered`: Boolean (TRUE náº¿u Ä‘Ã£ giao hÃ ng)

**Business Rules**:
```python
# TÃ­nh delivery metrics
approval_delay_days = datediff(approved_at, purchase_timestamp)
actual_delivery_days = datediff(delivered_customer_date, purchase_timestamp)
estimated_delivery_days = datediff(estimated_delivery_date, purchase_timestamp)

# Delivery delay (khÃ´ng Ã¢m)
delivery_delay_days = greatest(
    datediff(delivered_customer_date, estimated_delivery_date), 
    lit(0)
)

# Late delivery flag
is_delivered_late = (delivery_delay_days > 0) & (order_status == 'delivered')
```

**Use Cases**:
- PhÃ¢n tÃ­ch on-time delivery rate
- Dá»± Ä‘oÃ¡n thá»i gian giao hÃ ng
- ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t logistics

#### 3. **olist_order_items** (112,650 records)
**Primary Key**: `order_id + order_item_id + product_id`

**New Columns**:
- `total_item_value`: Tá»•ng giÃ¡ trá»‹ item = `price + freight_value`
- `freight_ratio`: Tá»· lá»‡ phÃ­ váº­n chuyá»ƒn = `(freight_value / price) * 100`

**Transformations**:
```python
# TÃ­nh toÃ¡n giÃ¡ trá»‹
total_item_value = col("price") + col("freight_value")
freight_ratio = round((col("freight_value") / col("price")) * 100, 2)

# Deduplication
window = Window.partitionBy("order_id", "order_item_id", "product_id") \
               .orderBy(desc("processed_at"))
```

**Use Cases**:
- PhÃ¢n tÃ­ch giÃ¡ trá»‹ Ä‘Æ¡n hÃ ng
- Tá»‘i Æ°u hÃ³a chi phÃ­ váº­n chuyá»ƒn
- Pricing strategy

#### 4. **olist_order_payments** (103,886 records)
**Primary Key**: `order_id + payment_sequential`

**New Columns**:
- `installment_value`: GiÃ¡ trá»‹ má»—i ká»³ tráº£ gÃ³p = `payment_value / payment_installments`
- `is_installment_payment`: Boolean (TRUE náº¿u `payment_installments > 1`)

**Transformations**:
```python
# TÃ­nh giÃ¡ trá»‹ tráº£ gÃ³p
installment_value = when(
    col("payment_installments") > 0,
    round(col("payment_value") / col("payment_installments"), 2)
).otherwise(col("payment_value"))

# Flag tráº£ gÃ³p
is_installment_payment = col("payment_installments") > 1
```

**Payment Types**:
- `credit_card`: Tháº» tÃ­n dá»¥ng (phá»• biáº¿n nháº¥t)
- `boleto`: PhÆ°Æ¡ng thá»©c thanh toÃ¡n Brazil
- `voucher`: Voucher/gift card
- `debit_card`: Tháº» ghi ná»£

**Use Cases**:
- PhÃ¢n tÃ­ch phÆ°Æ¡ng thá»©c thanh toÃ¡n
- Dá»± Ä‘oÃ¡n rá»§i ro tÃ i chÃ­nh
- Customer segmentation by payment behavior

#### 5. **olist_order_reviews** (100,000 records)
**Primary Key**: `review_id`

**New Columns**:
- `review_sentiment`: PhÃ¢n loáº¡i cáº£m xÃºc
  - `POSITIVE`: review_score = 4 hoáº·c 5
  - `NEUTRAL`: review_score = 3
  - `NEGATIVE`: review_score = 1 hoáº·c 2
- `has_comment`: Boolean (TRUE náº¿u cÃ³ `review_comment_message`)
- `review_response_time_hours`: Thá»i gian pháº£n há»“i (review_answer_timestamp - review_creation_date)

**Transformations**:
```python
# Sentiment classification
review_sentiment = when(col("review_score") >= 4, "POSITIVE") \
                  .when(col("review_score") == 3, "NEUTRAL") \
                  .otherwise("NEGATIVE")

# Comment flag
has_comment = col("review_comment_message").isNotNull()

# Response time (hours)
review_response_time_hours = round(
    (unix_timestamp("review_answer_timestamp") - 
     unix_timestamp("review_creation_date")) / 3600,
    2
)
```

**Sentiment Distribution** (Æ°á»›c tÃ­nh):
- ðŸŸ¢ POSITIVE (4-5 stars): ~77%
- ðŸŸ¡ NEUTRAL (3 stars): ~11%
- ðŸ”´ NEGATIVE (1-2 stars): ~12%

**Use Cases**:
- Customer satisfaction analysis
- Seller performance evaluation
- Product quality insights
- Response time optimization

#### 6. **olist_products** (32,951 records)
**Primary Key**: `product_id`

**New Columns**:
- `product_volume_cm3`: Thá»ƒ tÃ­ch sáº£n pháº©m = `length_cm Ã— height_cm Ã— width_cm`

**Transformations**:
```python
# Cast dimensions to Integer
length_cm = col("product_length_cm").cast(IntegerType())
height_cm = col("product_height_cm").cast(IntegerType())
width_cm = col("product_width_cm").cast(IntegerType())

# Calculate volume
product_volume_cm3 = length_cm * height_cm * width_cm

# Validation: positive values only
volume = when(product_volume_cm3 > 0, product_volume_cm3).otherwise(None)
```

**Use Cases**:
- Shipping cost optimization
- Warehouse space planning
- Product categorization by size

#### 7. **olist_sellers** (3,095 records)
**Primary Key**: `seller_id`

**Transformations**:
- âœ… Uppercase: `seller_city`, `seller_state`
- âœ… Trim whitespace
- âœ… Deduplication by `seller_id`

**Geographic Distribution**:
- Top state: SP (SÃ£o Paulo) - largest seller base
- Urban concentration in major cities

#### 8. **olist_geolocation** (19,015 records - giáº£m 98.1%)
**Primary Key**: `geolocation_zip_code_prefix`

**Transformations**:
```python
# Aggregation strategy
geo_agg = bronze_df.groupBy(
    "geolocation_zip_code_prefix",
    "geolocation_city", 
    "geolocation_state"
).agg(
    avg("geolocation_lat").alias("geolocation_lat"),
    avg("geolocation_lng").alias("geolocation_lng")
)

# Type casting
lat = col("geolocation_lat").cast(DoubleType())
lng = col("geolocation_lng").cast(DoubleType())

# Validation
lat_valid = (lat >= -90) & (lat <= 90)
lng_valid = (lng >= -180) & (lng <= 180)
```

**Size Reduction**:
- Before: 1,000,516 records (raw coordinates)
- After: 19,015 records (aggregated by zip code)
- Reduction: 98.1% (avglatitude/longitude per zip)

**Use Cases**:
- Geospatial analysis
- Delivery route optimization
- Regional market analysis

#### 9. **olist_product_category_translation** (71 records)
**Primary Key**: `product_category_name`

**Transformations**:
- âœ… Trim whitespace
- âœ… Lowercase normalization cho category names
- âœ… Deduplication by `product_category_name`
- âœ… Mapping Portuguese â†’ English category names

**Sample Translations**:
```
beleza_saude          â†’ health_beauty
informatica_acessorios â†’ computers_accessories
moveis_decoracao       â†’ furniture_decor
```

### Data Quality Validation

**Script**: `spark/app/validate_silver_quality.py`

**Kiá»ƒm tra tá»± Ä‘á»™ng**:
1. âœ… **Primary Key Validation**: No NULL values in primary keys
2. âœ… **Duplicate Detection**: No duplicate records per table
3. âœ… **NULL Analysis**: NULL rate per column
4. âœ… **Business Rules**:
   - State codes: 2 letters, uppercase
   - Zip codes: 5 digits
   - Coordinates: Valid lat/lng ranges
   - Prices: Positive values
   - Review scores: 1-5 range

**Cháº¡y validation**:
```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --master local[*] \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/validate_silver_quality.py
```

### Truy váº¥n Silver Layer

**Via PySpark**:
```bash
# Show customers data
docker exec spark-master /opt/spark/bin/spark-submit \
  --master local[*] \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/show_customers.py
```

### 10. Data Quality Validation
```bash
# Run validation checks on Silver layer
docker exec spark-master /opt/spark/bin/spark-submit \
  --master local[*] \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/validate_silver_quality.py
```

**Via MinIO Console**:
```
URL: http://localhost:9001
Credentials: admin / password123
Navigate: Buckets â†’ silver â†’ [table_name]
```

**Via Trino SQL**:
```sql
-- Connect to Trino
docker exec -it trino trino --catalog iceberg --schema default

-- Query Silver tables
SELECT 
    customer_state,
    COUNT(*) as customer_count
FROM iceberg.default.olist_customers
GROUP BY customer_state
ORDER BY customer_count DESC
LIMIT 10;
```

### Delta Lake Features

**Time Travel**:
```python
# Read specific version
df = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .load("s3a://silver/olist_customers")

# Read as of timestamp
df = spark.read.format("delta") \
    .option("timestampAsOf", "2025-11-19 12:00:00") \
    .load("s3a://silver/olist_customers")
```

**Transaction Log**:
```bash
# View Delta log
docker exec -it minio-client mc cat \
  local/silver/olist_customers/_delta_log/00000000000000000002.json
```

### Silver Layer Statistics

**Storage Breakdown**:
```
Total Size: 109 MiB
â”œâ”€â”€ olist_order_items:     ~25 MiB (22%)
â”œâ”€â”€ olist_orders:          ~22 MiB (20%)
â”œâ”€â”€ olist_customers:       ~18 MiB (17%)
â”œâ”€â”€ olist_order_payments:  ~16 MiB (15%)
â”œâ”€â”€ olist_order_reviews:   ~15 MiB (14%)
â”œâ”€â”€ olist_products:        ~7 MiB (6%)
â”œâ”€â”€ olist_geolocation:     ~4 MiB (4%)
â”œâ”€â”€ olist_sellers:         ~2 MiB (2%)
â””â”€â”€ product_translation:   <1 MiB (<1%)
```

**Processing Time**:
- Full batch: 3-5 minutes (all 9 tables)
- Single table: 30-60 seconds
- Incremental: <1 minute (with proper partitioning)

**Data Lineage**:
```
PostgreSQL (Source)
    â†“ Debezium CDC
Kafka Topics (Stream)
    â†“ Kafka Connect
Bronze Layer (Parquet)
    â†“ Spark ETL
Silver Layer (Delta Lake)
    â†“ Spark Aggregation
Gold Layer (Analytical Tables) [TODO]
```

## âš™ï¸ Airflow Orchestration

### DAG: bronze_to_silver_processing

**Schedule**: Daily at 00:00 UTC (`@daily`)

**Tasks**:
1. `log_start`: Log khá»Ÿi Ä‘á»™ng
2. `process_bronze_to_silver`: Cháº¡y Spark job
3. `log_completion`: Log hoÃ n thÃ nh

**Configuration**:
```python
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 19),
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}
```

**Execution**:
```bash
# Via Airflow UI
http://localhost:8081

# Via CLI
docker exec airflow airflow dags trigger bronze_to_silver_processing
```

## ðŸ” Truy cáº­p cÃ¡c dá»‹ch vá»¥

| Service | URL | Credentials | Má»¥c Ä‘Ã­ch |
|---------|-----|-------------|----------|
| **Airflow** | http://localhost:8081 | hoang / 123456 | Workflow management |
| **MinIO Console** | http://localhost:9001 | admin / password123 | Data browser |
| **Spark Master UI** | http://localhost:8080 | - | Spark monitoring |
| **Kafka UI** | http://localhost:8084 | - | Kafka topics/consumers |
| **Trino UI** | http://localhost:8082 | - | Query execution |
| **Metabase** | http://localhost:3000 | - | BI dashboard |
| **Debezium Connect** | http://localhost:8083 | - | CDC connector status |

## ðŸ“ˆ Thá»‘ng kÃª dá»¯ liá»‡u

### Tá»•ng quan Silver Layer
```
Total Records: ~570,000
Total Size: 109 MiB (131 objects)
Format: Delta Lake (ACID compliant)
Partitioning: None (can be added later)
Compression: Snappy
```

### Record counts by table
```
â”œâ”€â”€ olist_customers:                     99,441 (17.4%)
â”œâ”€â”€ olist_orders:                        99,441 (17.4%)
â”œâ”€â”€ olist_order_items:                  112,650 (19.8%)
â”œâ”€â”€ olist_order_payments:               103,886 (18.2%)
â”œâ”€â”€ olist_order_reviews:                100,000 (17.5%)
â”œâ”€â”€ olist_products:                      32,951 (5.8%)
â”œâ”€â”€ olist_sellers:                        3,095 (0.5%)
â”œâ”€â”€ olist_geolocation:                   19,015 (3.3%)
â””â”€â”€ olist_product_category_translation:      71 (0.01%)
```

## ðŸ”§ Troubleshooting

### 1. Services khÃ´ng start
```bash
# Check logs
docker-compose logs [service_name]

# Restart specific service
docker-compose restart [service_name]

# Full restart
docker-compose down
docker-compose up -d
```

### 2. CDC khÃ´ng hoáº¡t Ä‘á»™ng
```bash
# Check connector status
curl http://localhost:8083/connectors/postgres-olist-source/status

# Restart connector
curl -X POST http://localhost:8083/connectors/postgres-olist-source/restart

# Check Kafka topics
docker exec kafka kafka-topics.sh --bootstrap-server kafka:9092 --list
```

### 3. Spark job failed
```bash
# Check Spark logs
docker logs spark-master

# Check MinIO accessibility
docker exec spark-master curl http://minio:9000/minio/health/live

# Verify credentials
docker exec spark-master env | grep AWS
```

### 4. Airflow DAG failed
```bash
# Check Airflow logs
docker logs airflow

# Check Docker socket access
docker exec airflow docker ps

# Manually trigger
docker exec airflow airflow dags trigger bronze_to_silver_processing
```

## ðŸ›‘ Táº¯t há»‡ thá»‘ng

```bash
# Stop (giá»¯ data)
docker-compose stop

# Start láº¡i
docker-compose start

# XÃ³a containers (giá»¯ data)
docker-compose down

# XÃ³a táº¥t cáº£ (bao gá»“m data)
docker-compose down -v
```

## ðŸ” Báº£o máº­t dá»¯ liá»‡u

### Volumes Ä‘Æ°á»£c preserve:
- `./postgres_data`: PostgreSQL database
- `./minio_data`: Bronze & Silver layers
- `./airflow/logs`: Airflow execution history

### Backup recommendation:
```bash
# Backup MinIO data
tar -czf minio_backup_$(date +%Y%m%d).tar.gz minio_data/

# Backup PostgreSQL
docker exec postgres pg_dump -U postgres orders > backup_$(date +%Y%m%d).sql
```

## ðŸ“š TÃ i liá»‡u tham kháº£o

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Delta Lake Guide](https://docs.delta.io/latest/index.html)
- [Debezium PostgreSQL Connector](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [MinIO Documentation](https://min.io/docs/minio/linux/index.html)



