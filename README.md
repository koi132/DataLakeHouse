# DataLakeHouse - Olist E-commerce Analytics

## üìã T·ªïng quan d·ª± √°n

H·ªá th·ªëng Data Lakehouse x·ª≠ l√Ω v√† ph√¢n t√≠ch d·ªØ li·ªáu th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ Olist s·ª≠ d·ª•ng ki·∫øn tr√∫c Medallion (Bronze ‚Üí Silver ‚Üí Gold) v·ªõi Apache Spark, Delta Lake, v√† MinIO.

### üéØ M·ª•c ti√™u

- X√¢y d·ª±ng pipeline ETL t·ª± ƒë·ªông h√≥a v·ªõi Change Data Capture (CDC)
- √Åp d·ª•ng ki·∫øn tr√∫c Medallion ƒë·ªÉ qu·∫£n l√Ω ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu
- Thi·∫øt k·∫ø Star Schema cho ph√¢n t√≠ch d·ªØ li·ªáu
- T√≠ch h·ª£p c√°c c√¥ng c·ª• BI/Analytics hi·ªán ƒë·∫°i

### üèóÔ∏è Ki·∫øn tr√∫c h·ªá th·ªëng

![architecture](./doc/architecture.png)

### üõ†Ô∏è Stack c√¥ng ngh·ªá

| Th√†nh ph·∫ßn | C√¥ng ngh·ªá | Port |
|------------|-----------|------|
| Source DB | PostgreSQL | 5432 |
| CDC | Debezium Connect | 8083 |
| Messaging | Apache Kafka | 9092 |
| Storage | MinIO | 9000/9001 |
| Processing | Apache Spark | 7077/8080 |
| Query Engine | Trino | 8082 |
| Orchestration | Apache Airflow | 8081 |
| BI | Metabase | 3000 |
| Monitoring | Kafka UI | 8084 |
| API & Chatbot | FastAPI + Gemini AI | 8000 |

---

## üöÄ H∆∞·ªõng d·∫´n ch·∫°y t·ª´ng b∆∞·ªõc

### B∆∞·ªõc 1: Kh·ªüi ƒë·ªông h·ªá th·ªëng

```bash
# Clone repository
git clone https://github.com/koi132/DataLakeHouse.git
cd DataLakeHouse

# Start all services
docker-compose up --build -d

# Verify services
docker ps
```

### B∆∞·ªõc 2: T·∫°o databases

```bash
# Create Airflow database
docker exec -it postgres psql -U postgres -c "CREATE DATABASE airflow;"

# Create Metabase database
docker exec -it postgres psql -U postgres -c "CREATE DATABASE metabase;"
```

### B∆∞·ªõc 3: Import d·ªØ li·ªáu v√†o PostgreSQL

```bash
# Copy dataset v√†o container
docker cp dataset/ecommerce/. postgres:/tmp/

# Copy SQL scripts
docker cp Script/. postgres:/tmp/

# T·∫°o tables
docker exec -it postgres psql -U postgres -d orders -f /tmp/create_tables.sql

# Import data
docker exec -it postgres psql -U postgres -d orders -f /tmp/import_raw.sql
```

### B∆∞·ªõc 4: ƒêƒÉng k√Ω Debezium CDC Connector

**PowerShell:**

```powershell
curl.exe -X POST http://localhost:8083/connectors `
  -H "Content-Type: application/json" `
  -d "@e:\Projects\DataLakeHouse\connectors\postgres-olist-initial.json"
```

**Bash/CMD:**

```bash
curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d "@connectors/postgres-olist-initial.json"
```

### B∆∞·ªõc 5: Ch·∫°y Kafka ‚Üí Bronze ETL

```bash
# V√†o Spark container
docker exec -it spark-master bash

# Submit batch job
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/process_kafka_to_bronze.py
```

### B∆∞·ªõc 6: Ch·∫°y Bronze ‚Üí Silver ETL

```bash
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/process_bronze_to_silver.py
```

### B∆∞·ªõc 7: Ch·∫°y Silver ‚Üí Gold ETL

```bash
/opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --packages io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  /opt/spark/app/process_silver_to_gold.py
```

---
### Galaxy Schema

![architecture](./doc/schema.png)

---

## üîç Truy c·∫≠p Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **FastAPI Chat** | http://localhost:8000 | - |
| Airflow | http://localhost:8081 | airflow / airflow |
| MinIO Console | http://localhost:9001 | admin / password123 |
| Spark Master UI | http://localhost:8080 | - |
| Kafka UI | http://localhost:8084 | - |
| Trino UI | http://localhost:8082 | - |
| Metabase | http://localhost:3000 | - |

---

## üìä D·ªØ li·ªáu

### Bronze Layer

- Raw CDC data t·ª´ Kafka
- Format: Delta Lake
- Location: `s3a://bronze/`

### Silver Layer

- Cleaned & transformed data
- Deduplication, type casting, business rules
- Location: `s3a://silver/`

### Gold Layer

- Star Schema (Dimensions + Facts)
- Optimized for analytics
- Location: `s3a://gold/`

**Dimension Tables:** dim_geography, dim_date, dim_customer, dim_seller, dim_product, dim_order_status

**Fact Tables:** fact_order_items, fact_reviews

---

## ü§ñ API & AI Chatbot

### T·ªïng quan

H·ªá th·ªëng cung c·∫•p REST API v√† AI Chatbot ƒë·ªÉ truy v·∫•n d·ªØ li·ªáu t·ª´ Data Lakehouse th√¥ng qua Trino.


### Truy c·∫≠p

| Service | URL | M√¥ t·∫£ |
|---------|-----|-------|
| Chat UI | http://localhost:8000 | Giao di·ªán chat v·ªõi AI |
| API Docs | http://localhost:8000/docs | Swagger documentation |
| API List | http://localhost:8000/apis | Danh s√°ch API c√≥ s·∫µn |

### API Endpoints

#### Data APIs

```bash
# L·∫•y s·ªë l∆∞·ª£ng kh√°ch h√†ng
GET http://localhost:8000/api/v1/cus_cnt

# L·∫•y s·ªë l∆∞·ª£ng s·∫£n ph·∫©m  
GET http://localhost:8000/api/v1/prd_cnt

# L·∫•y schema c·ªßa API
GET http://localhost:8000/api/v1/{api_name}/schema
```

#### Chatbot APIs

```bash
# Chat v·ªõi AI
POST http://localhost:8000/chat
Content-Type: application/json
{
    "message": "C√≥ bao nhi√™u kh√°ch h√†ng?"
}

# Reset chat history
POST http://localhost:8000/chat/reset

# Xem l·ªãch s·ª≠ chat
GET http://localhost:8000/chat/history
```

### C·∫•u h√¨nh Gemini API Key

1. L·∫•y API key t·ª´: https://aistudio.google.com/app/apikey

2. M·ªü file `api/core/chatbot.py` v√† thay th·∫ø API key:

```python
GEMINI_API_KEY = "your_api_key_here"  # Thay b·∫±ng key c·ªßa b·∫°n
```

### Th√™m API m·ªõi

1. T·∫°o file SQL trong `api/sql/`:
```sql
-- api/sql/my_query.sql
SELECT * FROM delta.gold.dim_customer LIMIT 100
```

2. API t·ª± ƒë·ªông available t·∫°i:
```
GET http://localhost:8000/api/v1/my_query
```



